{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":256618,"sourceType":"datasetVersion","datasetId":107620},{"sourceId":639622,"sourceType":"datasetVersion","datasetId":316368},{"sourceId":653195,"sourceType":"datasetVersion","datasetId":325566},{"sourceId":671851,"sourceType":"datasetVersion","datasetId":338555}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#  <center> Speech Emotion Recognition <center>","metadata":{}},{"cell_type":"markdown","source":"## Importing Libraries","metadata":{}},{"cell_type":"code","source":"from transformers import AutoFeatureExtractor, ASTForAudioClassification, TrainingArguments, Trainer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom datasets import Dataset\nimport pandas as pd\nimport torch\nimport librosa\nimport os\nimport numpy as np\nimport random\nfrom tqdm import tqdm\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import Audio\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-07-26T17:23:28.645016Z","iopub.execute_input":"2024-07-26T17:23:28.645481Z","iopub.status.idle":"2024-07-26T17:23:35.413704Z","shell.execute_reply.started":"2024-07-26T17:23:28.645449Z","shell.execute_reply":"2024-07-26T17:23:35.412706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load and Pre Process the Dataset","metadata":{}},{"cell_type":"code","source":"# Paths for datasets\nRavdess_path = \"/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/\"\nCrema_path = \"/kaggle/input/cremad/AudioWAV/\"\nTess_path = \"/kaggle/input/toronto-emotional-speech-set-tess/tess toronto emotional speech set data/TESS Toronto emotional speech set data/\"\nSavee_path = \"/kaggle/input/surrey-audiovisual-expressed-emotion-savee/ALL/\"","metadata":{"execution":{"iopub.status.busy":"2024-07-26T17:23:35.415499Z","iopub.execute_input":"2024-07-26T17:23:35.416034Z","iopub.status.idle":"2024-07-26T17:23:35.420739Z","shell.execute_reply.started":"2024-07-26T17:23:35.416006Z","shell.execute_reply":"2024-07-26T17:23:35.419672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to process RAVDESS dataset\ndef process_ravdess(Ravdess_path):\n    ravdess_directory_list = os.listdir(Ravdess_path)\n    file_emotion = []\n    file_path = []\n\n    for dir in ravdess_directory_list:\n        actor_dir = os.path.join(Ravdess_path, dir)\n        if os.path.isdir(actor_dir):\n            actor = os.listdir(actor_dir)\n            for file in actor:\n                part = file.split('.')[0]\n                part = part.split('-')\n                file_emotion.append(int(part[2]))\n                file_path.append(os.path.join(actor_dir, file))\n                \n    emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n    path_df = pd.DataFrame(file_path, columns=['Path'])\n    Ravdess_df = pd.concat([emotion_df, path_df], axis=1)\n    Ravdess_df.Emotions.replace({1:'neutral', 2:'calm', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'}, inplace=True)\n    return Ravdess_df","metadata":{"execution":{"iopub.status.busy":"2024-07-26T17:23:35.421986Z","iopub.execute_input":"2024-07-26T17:23:35.422351Z","iopub.status.idle":"2024-07-26T17:23:35.437507Z","shell.execute_reply.started":"2024-07-26T17:23:35.422315Z","shell.execute_reply":"2024-07-26T17:23:35.436758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to process CREMA dataset\ndef process_crema(Crema_path):\n    crema_directory_list = os.listdir(Crema_path)\n    file_emotion = []\n    file_path = []\n\n    for file in crema_directory_list:\n        file_path.append(os.path.join(Crema_path, file))\n        part = file.split('_')\n        if part[2] == 'SAD':\n            file_emotion.append('sad')\n        elif part[2] == 'ANG':\n            file_emotion.append('angry')\n        elif part[2] == 'DIS':\n            file_emotion.append('disgust')\n        elif part[2] == 'FEA':\n            file_emotion.append('fear')\n        elif part[2] == 'HAP':\n            file_emotion.append('happy')\n        elif part[2] == 'NEU':\n            file_emotion.append('neutral')\n        else:\n            file_emotion.append('Unknown')\n            \n    emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n    path_df = pd.DataFrame(file_path, columns=['Path'])\n    Crema_df = pd.concat([emotion_df, path_df], axis=1)\n    return Crema_df","metadata":{"execution":{"iopub.status.busy":"2024-07-26T17:23:35.439471Z","iopub.execute_input":"2024-07-26T17:23:35.439784Z","iopub.status.idle":"2024-07-26T17:23:35.453456Z","shell.execute_reply.started":"2024-07-26T17:23:35.439754Z","shell.execute_reply":"2024-07-26T17:23:35.452672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to process TESS dataset\ndef process_tess(Tess_path):\n    tess_directory_list = os.listdir(Tess_path)\n    file_emotion = []\n    file_path = []\n\n    for dir in tess_directory_list:\n        dir_path = os.path.join(Tess_path, dir)\n        if os.path.isdir(dir_path):\n            directories = os.listdir(dir_path)\n            for file in directories:\n                file_path.append(os.path.join(dir_path, file))\n                part = file.split('.')[0]\n                part = part.split('_')[2]\n                if part == 'ps':\n                    file_emotion.append('surprise')\n                else:\n                    file_emotion.append(part)\n                \n    emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n    path_df = pd.DataFrame(file_path, columns=['Path'])\n    Tess_df = pd.concat([emotion_df, path_df], axis=1)\n    return Tess_df\n","metadata":{"execution":{"iopub.status.busy":"2024-07-26T17:23:35.454495Z","iopub.execute_input":"2024-07-26T17:23:35.454793Z","iopub.status.idle":"2024-07-26T17:23:35.463713Z","shell.execute_reply.started":"2024-07-26T17:23:35.454769Z","shell.execute_reply":"2024-07-26T17:23:35.462880Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to process SAVEE dataset\ndef process_savee(Savee_path):\n    savee_directory_list = os.listdir(Savee_path)\n    file_emotion = []\n    file_path = []\n\n    for file in savee_directory_list:\n        file_path.append(os.path.join(Savee_path, file))\n        part = file.split('_')[1]\n        ele = part[:-6]\n        if ele == 'a':\n            file_emotion.append('angry')\n        elif ele == 'd':\n            file_emotion.append('disgust')\n        elif ele == 'f':\n            file_emotion.append('fear')\n        elif ele == 'h':\n            file_emotion.append('happy')\n        elif ele == 'n':\n            file_emotion.append('neutral')\n        elif ele == 'sa':\n            file_emotion.append('sad')\n        else:\n            file_emotion.append('surprise')\n            \n    emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n    path_df = pd.DataFrame(file_path, columns=['Path'])\n    Savee_df = pd.concat([emotion_df, path_df], axis=1)\n    return Savee_df","metadata":{"execution":{"iopub.status.busy":"2024-07-26T17:23:35.464776Z","iopub.execute_input":"2024-07-26T17:23:35.465046Z","iopub.status.idle":"2024-07-26T17:23:35.478813Z","shell.execute_reply.started":"2024-07-26T17:23:35.465023Z","shell.execute_reply":"2024-07-26T17:23:35.477901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Process each dataset\nRavdess_df = process_ravdess(Ravdess_path)\nCrema_df = process_crema(Crema_path)\nTess_df = process_tess(Tess_path)\nSavee_df = process_savee(Savee_path)\n\n# Combine all dataframes\ndf = pd.concat([Ravdess_df, Crema_df, Tess_df, Savee_df], axis=0)\n\n# Encode emotion labels\nlabel_encoder = LabelEncoder()\nencoded_labels = label_encoder.fit_transform(df['Emotions'])\n\n# Split dataset into train and test sets with stratification\ntrain_paths, test_paths, train_labels, test_labels = train_test_split(\n    df['Path'].tolist(), encoded_labels, test_size=0.2, random_state=42, stratify=encoded_labels\n)\n\nprint(f\"Number of training samples: {len(train_paths)}\")\nprint(f\"Number of testing samples: {len(test_paths)}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-26T17:23:35.479887Z","iopub.execute_input":"2024-07-26T17:23:35.480495Z","iopub.status.idle":"2024-07-26T17:23:35.589276Z","shell.execute_reply.started":"2024-07-26T17:23:35.480466Z","shell.execute_reply":"2024-07-26T17:23:35.588446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2024-07-26T17:23:35.590444Z","iopub.execute_input":"2024-07-26T17:23:35.590710Z","iopub.status.idle":"2024-07-26T17:23:35.604601Z","shell.execute_reply.started":"2024-07-26T17:23:35.590687Z","shell.execute_reply":"2024-07-26T17:23:35.603758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# Find unique values in Emotions\nunique_values_column1 = df['Emotions'].unique()\nprint(unique_values_column1) ","metadata":{"execution":{"iopub.status.busy":"2024-07-26T17:23:35.605819Z","iopub.execute_input":"2024-07-26T17:23:35.606164Z","iopub.status.idle":"2024-07-26T17:23:35.612409Z","shell.execute_reply.started":"2024-07-26T17:23:35.606130Z","shell.execute_reply":"2024-07-26T17:23:35.611491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to load, resample, and preprocess audio\ndef load_and_preprocess_audio(audio_path, target_sampling_rate):\n    try:\n        audio, sr = librosa.load(audio_path, sr=None)  # Load audio with original sampling rate\n        if sr != target_sampling_rate:\n            audio = librosa.resample(audio, orig_sr=sr, target_sr=target_sampling_rate)  # Resample to target rate\n    except Exception as e:\n        print(f\"Error loading audio file {audio_path}: {e}\")\n        return None\n    return audio","metadata":{"execution":{"iopub.status.busy":"2024-07-26T17:23:35.616753Z","iopub.execute_input":"2024-07-26T17:23:35.617099Z","iopub.status.idle":"2024-07-26T17:23:35.622944Z","shell.execute_reply.started":"2024-07-26T17:23:35.617065Z","shell.execute_reply":"2024-07-26T17:23:35.621969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Visualisation and Exploration","metadata":{}},{"cell_type":"markdown","source":"First let's plot the count of each emotions in our dataset.","metadata":{}},{"cell_type":"code","source":"# Assuming you have a DataFrame named Savee_df\nplt.figure(figsize=(16, 6))\nplt.title('Count of Emotions', size=20)\nsns.countplot(data=df, x='Emotions', order=df['Emotions'].value_counts().index)\nplt.ylabel('Count', size=14)\nplt.xlabel('Emotions', size=14)\nsns.despine(top=True, right=True, left=False, bottom=False)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-26T17:23:35.623965Z","iopub.execute_input":"2024-07-26T17:23:35.624243Z","iopub.status.idle":"2024-07-26T17:23:35.949340Z","shell.execute_reply.started":"2024-07-26T17:23:35.624219Z","shell.execute_reply":"2024-07-26T17:23:35.948416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can also plot waveplots and spectograms for audio signals\n\n* Waveplots - Waveplots let us know the loudness of the audio at a given time.\n* Spectograms - A spectrogram is a visual representation of the spectrum of frequencies of sound or other signals as they vary with time. Itâ€™s a representation of frequencies changing with respect to time for given audio/music signals.","metadata":{}},{"cell_type":"code","source":"# Now, for visualization, you can use the following code:\n\ndef create_waveplot(data, sr, e):\n    plt.figure(figsize=(10, 3))\n    plt.title('Waveplot for audio with {} emotion'.format(e), size=15)\n    plt.plot(np.linspace(0, len(data) / sr, num=len(data)), data)\n    plt.xlabel('Time (s)')\n    plt.ylabel('Amplitude')\n    plt.show()\n\n\ndef create_spectrogram(data, sr, e):\n    # stft function converts the data into short term fourier transform\n    X = librosa.stft(data)\n    Xdb = librosa.amplitude_to_db(abs(X))\n    plt.figure(figsize=(12, 3))\n    plt.title('Spectrogram for audio with {} emotion'.format(e), size=15)\n    librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='hz')   \n    #librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='log')\n    plt.colorbar()","metadata":{"execution":{"iopub.status.busy":"2024-07-26T17:23:35.950522Z","iopub.execute_input":"2024-07-26T17:23:35.950834Z","iopub.status.idle":"2024-07-26T17:23:35.958102Z","shell.execute_reply.started":"2024-07-26T17:23:35.950808Z","shell.execute_reply":"2024-07-26T17:23:35.957106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emotion='fear'\npath = np.array(df.Path[df.Emotions == emotion])[1]\ndata, sampling_rate = librosa.load(path)\ncreate_waveplot(data, sampling_rate, emotion)\ncreate_spectrogram(data, sampling_rate, emotion)\nAudio(path)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T17:23:35.959072Z","iopub.execute_input":"2024-07-26T17:23:35.959323Z","iopub.status.idle":"2024-07-26T17:23:38.617252Z","shell.execute_reply.started":"2024-07-26T17:23:35.959302Z","shell.execute_reply":"2024-07-26T17:23:38.616337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emotion='angry'\npath = np.array(df.Path[df.Emotions == emotion])[1]\ndata, sampling_rate = librosa.load(path)\ncreate_waveplot(data, sampling_rate, emotion)\ncreate_spectrogram(data, sampling_rate, emotion)\nAudio(path)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T17:23:38.618391Z","iopub.execute_input":"2024-07-26T17:23:38.619010Z","iopub.status.idle":"2024-07-26T17:23:39.447174Z","shell.execute_reply.started":"2024-07-26T17:23:38.618979Z","shell.execute_reply":"2024-07-26T17:23:39.446256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emotion='sad'\npath = np.array(df.Path[df.Emotions == emotion])[1]\ndata, sampling_rate = librosa.load(path)\ncreate_waveplot(data, sampling_rate, emotion)\ncreate_spectrogram(data, sampling_rate, emotion)\nAudio(path)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T17:23:39.448414Z","iopub.execute_input":"2024-07-26T17:23:39.448769Z","iopub.status.idle":"2024-07-26T17:23:40.255645Z","shell.execute_reply.started":"2024-07-26T17:23:39.448737Z","shell.execute_reply":"2024-07-26T17:23:40.254634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emotion='happy'\npath = np.array(df.Path[df.Emotions == emotion])[1]\ndata, sampling_rate = librosa.load(path)\ncreate_waveplot(data, sampling_rate, emotion)\ncreate_spectrogram(data, sampling_rate, emotion)\nAudio(path)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T17:23:40.256785Z","iopub.execute_input":"2024-07-26T17:23:40.257104Z","iopub.status.idle":"2024-07-26T17:23:41.082511Z","shell.execute_reply.started":"2024-07-26T17:23:40.257077Z","shell.execute_reply":"2024-07-26T17:23:41.081598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emotion='disgust'\npath = np.array(df.Path[df.Emotions == emotion])[1]\ndata, sampling_rate = librosa.load(path)\ncreate_waveplot(data, sampling_rate, emotion)\ncreate_spectrogram(data, sampling_rate, emotion)\nAudio(path)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T17:23:41.083880Z","iopub.execute_input":"2024-07-26T17:23:41.084560Z","iopub.status.idle":"2024-07-26T17:23:41.824505Z","shell.execute_reply.started":"2024-07-26T17:23:41.084526Z","shell.execute_reply":"2024-07-26T17:23:41.823630Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emotion='neutral'\npath = np.array(df.Path[df.Emotions == emotion])[1]\ndata, sampling_rate = librosa.load(path)\ncreate_waveplot(data, sampling_rate, emotion)\ncreate_spectrogram(data, sampling_rate, emotion)\nAudio(path)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T17:23:41.825615Z","iopub.execute_input":"2024-07-26T17:23:41.825906Z","iopub.status.idle":"2024-07-26T17:23:42.640082Z","shell.execute_reply.started":"2024-07-26T17:23:41.825881Z","shell.execute_reply":"2024-07-26T17:23:42.639150Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emotion='surprise'\npath = np.array(df.Path[df.Emotions == emotion])[1]\ndata, sampling_rate = librosa.load(path)\ncreate_waveplot(data, sampling_rate, emotion)\ncreate_spectrogram(data, sampling_rate, emotion)\nAudio(path)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T17:23:42.641277Z","iopub.execute_input":"2024-07-26T17:23:42.641564Z","iopub.status.idle":"2024-07-26T17:23:43.488769Z","shell.execute_reply.started":"2024-07-26T17:23:42.641539Z","shell.execute_reply":"2024-07-26T17:23:43.487889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emotion='calm'\npath = np.array(df.Path[df.Emotions == emotion])[1]\ndata, sampling_rate = librosa.load(path)\ncreate_waveplot(data, sampling_rate, emotion)\ncreate_spectrogram(data, sampling_rate, emotion)\nAudio(path)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T17:23:43.490161Z","iopub.execute_input":"2024-07-26T17:23:43.490804Z","iopub.status.idle":"2024-07-26T17:23:44.304702Z","shell.execute_reply.started":"2024-07-26T17:23:43.490767Z","shell.execute_reply":"2024-07-26T17:23:44.303783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Model","metadata":{}},{"cell_type":"code","source":"# Load the model and feature extractor\nfeature_extractor = AutoFeatureExtractor.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\nmodel = ASTForAudioClassification.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")","metadata":{"execution":{"iopub.status.busy":"2024-07-26T17:23:44.306048Z","iopub.execute_input":"2024-07-26T17:23:44.306400Z","iopub.status.idle":"2024-07-26T17:23:44.963184Z","shell.execute_reply.started":"2024-07-26T17:23:44.306366Z","shell.execute_reply":"2024-07-26T17:23:44.962323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check if a GPU is available and move the model to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-26T17:23:44.964348Z","iopub.execute_input":"2024-07-26T17:23:44.964625Z","iopub.status.idle":"2024-07-26T17:23:45.257366Z","shell.execute_reply.started":"2024-07-26T17:23:44.964602Z","shell.execute_reply":"2024-07-26T17:23:45.256472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Process","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\nimport torch\nfrom datasets import Dataset\n\ndef preprocess_data(file_paths, labels, feature_extractor, sampling_rate):\n    inputs = []\n    for i, file_path in enumerate(tqdm(file_paths, desc=\"Processing audio files\")):\n        audio = load_and_preprocess_audio(file_path, sampling_rate)\n        if audio is not None:\n            input_features = feature_extractor(audio, sampling_rate=sampling_rate, return_tensors=\"pt\")\n            input_features[\"labels\"] = torch.tensor([labels[i]])\n            inputs.append(input_features)\n    return inputs\n\n# Get the sampling rate from the feature extractor\nsampling_rate = feature_extractor.sampling_rate\n# Preprocess the train and test datasets\ntrain_dataset = preprocess_data(train_paths, train_labels.tolist(), feature_extractor, sampling_rate)\ntest_dataset = preprocess_data(test_paths, test_labels.tolist(), feature_extractor, sampling_rate)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T17:23:45.258366Z","iopub.execute_input":"2024-07-26T17:23:45.258615Z","iopub.status.idle":"2024-07-26T17:25:44.061014Z","shell.execute_reply.started":"2024-07-26T17:23:45.258593Z","shell.execute_reply":"2024-07-26T17:25:44.059973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nimport torch\n\nclass CustomDataset(Dataset):\n    def __init__(self, features):\n        self.features = features\n\n    def __len__(self):\n        return len(self.features)\n\n    def __getitem__(self, idx):\n        sample = self.features[idx]\n        # Ensure the sample has correct keys and values\n        input_values = sample[\"input_values\"].squeeze(0)  # Adjust if needed\n        labels = sample[\"labels\"].squeeze(0)  # Adjust if needed\n        return {\n            \"input_values\": input_values,\n            \"labels\": labels\n        }\n\n# Convert the lists of dictionaries to the CustomDataset format\ntrain_features = [{\"input_values\": x[\"input_values\"], \"labels\": x[\"labels\"]} for x in train_dataset]\ntest_features = [{\"input_values\": x[\"input_values\"], \"labels\": x[\"labels\"]} for x in test_dataset]\n\n# Wrap features into Dataset and use DataLoader\ntrain_dataset = CustomDataset(train_features)\ntest_dataset = CustomDataset(test_features)\n\ntrain_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-26T17:25:44.062774Z","iopub.execute_input":"2024-07-26T17:25:44.063592Z","iopub.status.idle":"2024-07-26T17:25:44.093066Z","shell.execute_reply.started":"2024-07-26T17:25:44.063563Z","shell.execute_reply":"2024-07-26T17:25:44.092126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to predict emotion from audio file\ndef predict_emotion(audio_path):\n    audio = load_and_preprocess_audio(audio_path, sampling_rate)\n    if audio is None:\n        return None\n\n    inputs = feature_extractor(audio, sampling_rate=sampling_rate, return_tensors=\"pt\")\n    inputs = {key: value.to(device) for key, value in inputs.items()}  # Move inputs to the same device as the model\n\n    with torch.no_grad():\n        logits = model(**inputs).logits\n\n    predicted_class_id = torch.argmax(logits, dim=-1).item()\n    predicted_label = model.config.id2label[predicted_class_id]\n    return predicted_label","metadata":{"execution":{"iopub.status.busy":"2024-07-26T17:25:44.094320Z","iopub.execute_input":"2024-07-26T17:25:44.094602Z","iopub.status.idle":"2024-07-26T17:25:44.106804Z","shell.execute_reply.started":"2024-07-26T17:25:44.094578Z","shell.execute_reply":"2024-07-26T17:25:44.105875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Update model config with custom labels\nmodel.config.id2label = {i: label for i, label in enumerate(label_encoder.classes_)}\nmodel.config.label2id = {label: i for i, label in enumerate(label_encoder.classes_)}\n\n# Get the sampling rate from the feature extractor or define a default value\ntry:\n    sampling_rate = feature_extractor.sampling_rate\nexcept AttributeError:\n    sampling_rate = 16000","metadata":{"execution":{"iopub.status.busy":"2024-07-26T17:25:44.108002Z","iopub.execute_input":"2024-07-26T17:25:44.108604Z","iopub.status.idle":"2024-07-26T17:25:44.117224Z","shell.execute_reply.started":"2024-07-26T17:25:44.108570Z","shell.execute_reply":"2024-07-26T17:25:44.116366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fine Tuning and Evaluation","metadata":{}},{"cell_type":"code","source":"# Training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    eval_strategy=\"epoch\",\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=3,\n    save_strategy=\"epoch\",\n    logging_dir=\"./logs\",\n)\n\n# Define the trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T17:25:44.118496Z","iopub.execute_input":"2024-07-26T17:25:44.119097Z","iopub.status.idle":"2024-07-26T17:25:44.788516Z","shell.execute_reply.started":"2024-07-26T17:25:44.119065Z","shell.execute_reply":"2024-07-26T17:25:44.787766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Wandb login:\nfrom kaggle_secrets import UserSecretsClient\nimport wandb\nuser_secrets = UserSecretsClient()\nsecret_value = user_secrets.get_secret(\"wandb_api_key\")\nwandb.login(key=secret_value)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T17:25:44.794116Z","iopub.execute_input":"2024-07-26T17:25:44.794376Z","iopub.status.idle":"2024-07-26T17:25:48.105182Z","shell.execute_reply.started":"2024-07-26T17:25:44.794353Z","shell.execute_reply":"2024-07-26T17:25:48.104279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Clear CUDA cache\ntorch.cuda.empty_cache()\n\n# Monitor GPU memory usage\ndef print_gpu_utilization():\n    print(f\"Allocated: {round(torch.cuda.memory_allocated(0)/1024**3,1)} GB\")\n    print(f\"Cached: {round(torch.cuda.memory_reserved(0)/1024**3,1)} GB\")\n\n# Print GPU utilization before training\nprint_gpu_utilization()","metadata":{"execution":{"iopub.status.busy":"2024-07-26T17:25:48.106258Z","iopub.execute_input":"2024-07-26T17:25:48.106835Z","iopub.status.idle":"2024-07-26T17:25:48.113105Z","shell.execute_reply.started":"2024-07-26T17:25:48.106808Z","shell.execute_reply":"2024-07-26T17:25:48.112172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the model\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-07-26T17:25:48.114445Z","iopub.execute_input":"2024-07-26T17:25:48.114977Z","iopub.status.idle":"2024-07-26T18:43:01.031237Z","shell.execute_reply.started":"2024-07-26T17:25:48.114950Z","shell.execute_reply":"2024-07-26T18:43:01.030130Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.evaluate()","metadata":{"execution":{"iopub.status.busy":"2024-07-26T18:43:01.033674Z","iopub.execute_input":"2024-07-26T18:43:01.034145Z","iopub.status.idle":"2024-07-26T18:44:47.982999Z","shell.execute_reply.started":"2024-07-26T18:43:01.034103Z","shell.execute_reply":"2024-07-26T18:44:47.982057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.save_model()","metadata":{"execution":{"iopub.status.busy":"2024-07-26T18:44:47.984137Z","iopub.execute_input":"2024-07-26T18:44:47.984398Z","iopub.status.idle":"2024-07-26T18:44:48.617587Z","shell.execute_reply.started":"2024-07-26T18:44:47.984375Z","shell.execute_reply":"2024-07-26T18:44:48.616492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference","metadata":{}},{"cell_type":"code","source":"# Set the model to evaluation mode\nmodel.eval()\n\n# Function to perform inference\ndef infer(model, dataloader, device):\n    model.to(device)  # Move model to the appropriate device\n    all_predictions = []\n    all_labels = []\n\n    with torch.no_grad():  # No need to compute gradients\n        for batch in tqdm(dataloader, desc=\"Inference\"):\n            input_values = batch[\"input_values\"].to(device)\n            labels = batch[\"labels\"].to(device)\n            \n            # Forward pass through the model\n            outputs = model(input_values)\n            \n            # Assuming model outputs logits\n            logits = outputs.logits  # Adjust if your model's output is different\n            predictions = torch.argmax(logits, dim=-1)  # Get predictions\n\n            all_predictions.extend(predictions.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    \n    return all_predictions, all_labels\n\n# Set the device for inference\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Perform inference on the test dataset\npredictions, true_labels = infer(model, test_loader, device)\n\n# Print example predictions and logits\nprint(\"Predicted Labels (raw):\", predictions)\nprint(\"True Labels:\", true_labels)\n\n# Check for out-of-range predictions\nunique_predictions = np.unique(predictions)\nprint(\"Unique Predictions:\", unique_predictions)\n\n# Filter out invalid predictions\nvalid_predictions = [pred for pred in predictions if pred in label_encoder.classes_]\ninvalid_predictions = [pred for pred in predictions if pred not in label_encoder.classes_]\n\nprint(\"Valid Predictions:\", valid_predictions)\nprint(\"Invalid Predictions:\", invalid_predictions)\n\n# Map predicted labels back to emotion names using label_encoder for valid predictions\nif valid_predictions:\n    predicted_emotions = label_encoder.inverse_transform(valid_predictions)\n    print(\"Predicted Emotions (valid):\", predicted_emotions)\nelse:\n    print(\"No valid predictions to map.\")\n\n# Optionally handle invalid predictions\n# This can be done by mapping them to a default class or logging them for analysis\nif invalid_predictions:\n    print(\"Handling invalid predictions...\")\n\n# Print example valid predictions\nprint(\"Predicted Labels (valid):\", valid_predictions)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-26T18:44:48.619230Z","iopub.execute_input":"2024-07-26T18:44:48.619608Z","iopub.status.idle":"2024-07-26T18:48:21.249516Z","shell.execute_reply.started":"2024-07-26T18:44:48.619573Z","shell.execute_reply":"2024-07-26T18:48:21.248295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prediction","metadata":{}},{"cell_type":"code","source":"# Example usage\nrandom_file = random.choice(test_paths)\nprint(f\"Selected file: {random_file}\")\n\npredicted_label = predict_emotion(random_file)\nprint(f\"Predicted Label: {predicted_label}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-26T18:48:21.251069Z","iopub.execute_input":"2024-07-26T18:48:21.251464Z","iopub.status.idle":"2024-07-26T18:48:21.346867Z","shell.execute_reply.started":"2024-07-26T18:48:21.251425Z","shell.execute_reply":"2024-07-26T18:48:21.345678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import IPython.display as ipd\n# Load the audio file for visualization and playback\naudio, sr = librosa.load(random_file, sr=None)  # Use None to preserve original sampling rate\n\n# Plot the waveform\nplt.figure(figsize=(12, 4))\nlibrosa.display.waveshow(audio, sr=sr)\nplt.title('Waveform')\nplt.xlabel('Time (s)')\nplt.ylabel('Amplitude')\nplt.show()\n\n# Plot the spectrogram\nplt.figure(figsize=(12, 4))\nspec = librosa.feature.melspectrogram(y=audio, sr=sr)\nspec_db = librosa.power_to_db(spec, ref=np.max)\nlibrosa.display.specshow(spec_db, sr=sr, x_axis='time', y_axis='mel')\nplt.colorbar(format='%+2.0f dB')\nplt.title('Mel Spectrogram')\nplt.xlabel('Time (s)')\nplt.ylabel('Frequency (Hz)')\nplt.show()\n\n# Play the audio\nprint(\"Audio Playback:\")\nipd.display(ipd.Audio(audio, rate=sr))","metadata":{"execution":{"iopub.status.busy":"2024-07-26T18:48:21.348817Z","iopub.execute_input":"2024-07-26T18:48:21.349168Z","iopub.status.idle":"2024-07-26T18:48:23.668375Z","shell.execute_reply.started":"2024-07-26T18:48:21.349133Z","shell.execute_reply":"2024-07-26T18:48:23.667447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Benchmarks","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import classification_report\nfrom sklearn.preprocessing import LabelEncoder\n\n# Assuming 'true_labels' and 'predictions' are already defined\n\n# Create a LabelEncoder and fit it with all possible labels\nall_labels = np.unique(np.concatenate([true_labels, predictions]))\nlabel_encoder = LabelEncoder()\nlabel_encoder.fit(all_labels)\n\n# Convert true labels and predictions to encoded form\ntrue_labels_encoded = label_encoder.transform(true_labels)\npredictions_encoded = label_encoder.transform(predictions)\n\n# Convert the numeric class labels to string labels\nclass_names = [str(label) for label in label_encoder.classes_]\n\n# Compute the classification report\nreport = classification_report(true_labels_encoded, predictions_encoded, target_names=class_names)\nprint(\"Classification Report:\")\nprint(report)\n\n\n# Compute and plot confusion matrix\nconf_matrix = confusion_matrix(true_labels, predictions)\nplt.figure(figsize=(10, 8))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.title(\"Confusion Matrix\")\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-07-26T18:48:23.669777Z","iopub.execute_input":"2024-07-26T18:48:23.670403Z","iopub.status.idle":"2024-07-26T18:48:24.147244Z","shell.execute_reply.started":"2024-07-26T18:48:23.670367Z","shell.execute_reply":"2024-07-26T18:48:24.146102Z"},"trusted":true},"execution_count":null,"outputs":[]}]}